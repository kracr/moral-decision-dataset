{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da44dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt creation function for generating new cases\n",
    "def generate_case_prompt(example):\n",
    "    example_json = json.dumps(example, indent=4)\n",
    "    return f\"\"\"\n",
    "  \n",
    "Behave as an instance generator. I will provide an example in JSON format. I need answer only in JSON format no unnessary comments needed in the output  \n",
    "For each example, generate five new instances that are similar in structure but distinct in the following aspects:  \n",
    "- Context: Use a different setting or scenario that aligns with real-world situations.  \n",
    "- Agents: Identify distinct active and passive agents with clearly defined roles and relationships.  \n",
    "- Ethical issues: Introduce new and realistic ethical dilemmas relevant to the scenario.  \n",
    "- Features: Maintain the same set of features as the original example, providing detailed, well-structured, and contextually accurate values.  \n",
    "But make sure that these features are not more than 2-3 words\n",
    "\n",
    "1. Ensure each generated instance explores diverse domains (e.g., healthcare, technology, education, business, law, etc.).  \n",
    "2. Clearly differentiate between the active agent (initiator of the action) and the passive agent (affected party).  \n",
    "3. Ensure the ethical issue is thought-provoking and aligns with the action and consequence.  \n",
    "4. Provide detailed descriptions for the consequence, its severity, utility, and duration.  \n",
    "5. Avoid repetitive or overly similar cases. Each instance should introduce fresh perspectives.  \n",
    "\n",
    "Here is an example in JSON format:\n",
    "{example_json}\n",
    "give the output as a commam seperated value format\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all JSON blocks from the response\n",
    "def extract_json_from_response(response_text):\n",
    "    try:\n",
    "        # Attempt to find all JSON-like blocks in the response\n",
    "        json_blocks = re.findall(r\"({.*?})\", response_text, re.DOTALL)\n",
    "        parsed_data = []\n",
    "        for block in json_blocks:\n",
    "            try:\n",
    "                parsed_data.append(json.loads(block))\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Skip invalid JSON blocks\n",
    "        return parsed_data if parsed_data else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JSON: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the agent and get a response\n",
    "def run_agent(client, prompt, model, content):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"assistant\", \"content\": content},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Extract response content\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        print(f\"Raw Response: {response_text}\")\n",
    "        # Extract and validate JSON\n",
    "        return extract_json_from_response(response_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in API call: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab031bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key for Together client\n",
    "os.environ['TOGETHER_API_KEY'] = \"Your_API_Key\"\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Define the LLM model\n",
    "llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_json(\"practice.json\")\n",
    "\n",
    "# Initialize lists to collect generated cases and invalid responses\n",
    "all_generated_cases = []\n",
    "invalid_responses = []\n",
    "\n",
    "# Process each example in the dataset\n",
    "for i in tqdm(range(len(data))):  # Iterate over all rows in the dataset\n",
    "    example = data.iloc[i].to_dict()  # Convert the current row to a dictionary\n",
    "\n",
    "    # Generate new cases based on the example\n",
    "    response_data = run_agent(\n",
    "        client,\n",
    "        generate_case_prompt(example),\n",
    "        llm_model,\n",
    "        \"You are a JSON instance generator and legal domain expert.\"\n",
    "    )\n",
    "\n",
    "    # Validate and append the response\n",
    "    if response_data:\n",
    "        all_generated_cases.extend(response_data)  # Add all parsed JSON objects\n",
    "    else:\n",
    "        invalid_responses.append({\"example_index\": i, \"example\": example})\n",
    "\n",
    "# Convert all generated cases into a DataFrame and save\n",
    "if all_generated_cases:\n",
    "    try:\n",
    "        results_df = pd.DataFrame(all_generated_cases)\n",
    "        results_df.to_csv(\"augmentation.csv\", index=False)\n",
    "        print(\"Generated cases saved as 'generated_cases.csv'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving generated cases to CSV: {e}\")\n",
    "else:\n",
    "    print(\"No valid cases generated.\")\n",
    "\n",
    "# Save invalid responses for debugging\n",
    "if invalid_responses:\n",
    "    invalid_output_file = \"/kaggle/working/invalid_responses.json\"\n",
    "    try:\n",
    "        with open(invalid_output_file, \"w\") as f:\n",
    "            json.dump(invalid_responses, f, indent=4)\n",
    "        print(f\"Invalid responses saved as '{invalid_output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving invalid responses: {e}\")\n",
    "else:\n",
    "    print(\"No invalid responses.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
