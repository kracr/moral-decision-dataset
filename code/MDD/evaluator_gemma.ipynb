{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42932c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "\n",
    "# Function to generate evaluation prompt\n",
    "def evaluation_prompt(summary, features):\n",
    "    return f\"\"\"\n",
    "    Evaluate the correctness of the following summary and features on a scale of 0 to 5 (0 = completely wrong, 5 = perfectly correct):\n",
    "\n",
    "    Summary: \"{summary}\"\n",
    "\n",
    "    Features: {json.dumps(features, indent=4)}\n",
    "\n",
    "    Instructions:\n",
    "    - Rate the SUMMARY's accuracy on a scale of 0 to 5.\n",
    "    - Rate the FEATURES' correctness on a scale of 0 to 5.\n",
    "    - Return only in this format:\n",
    "\n",
    "    Ratings:\n",
    "    - Summary: [0-5]\n",
    "    - Features: [0-5]\n",
    "\n",
    "    Feedback: Provide a brief explanation of the ratings.\n",
    "    \"\"\"\n",
    "\n",
    "# Function to run the evaluation agent\n",
    "def run_evaluation_agent(client, prompt, model):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a strict evaluator that rates accuracy of summaries and features.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in API call: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parse the ratings from the LLM response\n",
    "def parse_evaluation_response(response_text):\n",
    "    if not response_text:\n",
    "        return 0, 0, \"No response from the model.\"\n",
    "    try:\n",
    "        # Extract ratings for summary and features\n",
    "        summary_match = re.search(r\"Summary:\\s*\\[?([0-5])\\]?\", response_text, re.IGNORECASE)\n",
    "        features_match = re.search(r\"Features:\\s*\\[?([0-5])\\]?\", response_text, re.IGNORECASE)\n",
    "        feedback_match = re.search(r\"Feedback:\\s*(.*)\", response_text, re.DOTALL)\n",
    "\n",
    "        summary_rating = int(summary_match.group(1)) if summary_match else 0\n",
    "        features_rating = int(features_match.group(1)) if features_match else 0\n",
    "        feedback = feedback_match.group(1).strip() if feedback_match else \"No feedback provided.\"\n",
    "\n",
    "        return summary_rating, features_rating, feedback\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing evaluation response: {e}\")\n",
    "        return 0, 0, \"Parsing failure.\"\n",
    "\n",
    "# Main function to evaluate and add results to JSON\n",
    "def evaluate_json(input_json, output_json, client, llm_model):\n",
    "    try:\n",
    "        with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "        return\n",
    "\n",
    "    updated_results = []\n",
    "\n",
    "    for entry in tqdm(data):\n",
    "        case_id = entry.get(\"case_id\", \"unknown\")\n",
    "        selftext = entry.get(\"selftext\", \"\")\n",
    "        summary = entry.get(\"summary\", \"\")\n",
    "        features = {key: entry[key] for key in entry if key not in [\"case_id\", \"selftext\", \"summary\"]}\n",
    "\n",
    "        # Generate prompt for evaluation\n",
    "        prompt = evaluation_prompt(summary, features)\n",
    "\n",
    "        # Run evaluation agent\n",
    "        evaluation_response = run_evaluation_agent(client, prompt, llm_model)\n",
    "\n",
    "        # Parse response\n",
    "        summary_rating, features_rating, feedback = parse_evaluation_response(evaluation_response)\n",
    "\n",
    "        # Calculate overall rating\n",
    "        overall_rating = (summary_rating + features_rating) / 2\n",
    "\n",
    "        # Update entry with evaluation data\n",
    "        entry[\"summary_rating\"] = summary_rating\n",
    "        entry[\"features_rating\"] = features_rating\n",
    "        entry[\"overall_rating\"] = overall_rating\n",
    "        entry[\"feedback\"] = feedback\n",
    "\n",
    "        updated_results.append(entry)\n",
    "\n",
    "    # Save updated results\n",
    "    try:\n",
    "        with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(updated_results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Updated JSON with evaluations saved to {output_json}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output JSON: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['TOGETHER_API_KEY'] = \"your_api_key\"  # Replace with your Together API Key\n",
    "    client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "    llm_model = \"google/gemma-2-27b-it\"  # Replace with a valid Together-supported model\n",
    "\n",
    "    input_json = \"summary_and_features_flat.json\"  # Input JSON file\n",
    "    output_json = \"evaluated_summary_and_features.json\"  # Output JSON file\n",
    "\n",
    "    evaluate_json(input_json, output_json, client, llm_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
